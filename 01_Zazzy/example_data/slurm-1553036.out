Starting job 1553036 on c6-7 at Thu Nov 19 15:31:31 CET 2020

The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) StdEnv

R version 3.6.2 (2019-12-12) -- "Dark and Stormy Night"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #### newDADA2 script ####################################
> #### LUIS MORGADO 11 DEC 2019  #########################
> ####
> #### for 250 bp FASTERIS
> #### General PATH
> library(tidyverse)
-- Attaching packages --------------------------------------- tidyverse 1.3.0 --
v ggplot2 3.2.1     v purrr   0.3.4
v tibble  2.1.3     v dplyr   0.8.3
v tidyr   1.0.0     v stringr 1.4.0
v readr   1.3.1     v forcats 0.4.0
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
> library(ggplot2)
> library(dada2)
Loading required package: Rcpp
> library(ShortRead)
Loading required package: BiocGenerics
Loading required package: parallel

Attaching package: 'BiocGenerics'

The following objects are masked from 'package:parallel':

    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,
    clusterExport, clusterMap, parApply, parCapply, parLapply,
    parLapplyLB, parRapply, parSapply, parSapplyLB

The following objects are masked from 'package:dplyr':

    combine, intersect, setdiff, union

The following objects are masked from 'package:stats':

    IQR, mad, sd, var, xtabs

The following objects are masked from 'package:base':

    Filter, Find, Map, Position, Reduce, anyDuplicated, append,
    as.data.frame, basename, cbind, colnames, dirname, do.call,
    duplicated, eval, evalq, get, grep, grepl, intersect, is.unsorted,
    lapply, mapply, match, mget, order, paste, pmax, pmax.int, pmin,
    pmin.int, rank, rbind, rownames, sapply, setdiff, sort, table,
    tapply, union, unique, unsplit, which, which.max, which.min

Loading required package: BiocParallel
Loading required package: Biostrings
Loading required package: S4Vectors
Loading required package: stats4

Attaching package: 'S4Vectors'

The following objects are masked from 'package:dplyr':

    first, rename

The following object is masked from 'package:tidyr':

    expand

The following object is masked from 'package:base':

    expand.grid

Loading required package: IRanges

Attaching package: 'IRanges'

The following objects are masked from 'package:dplyr':

    collapse, desc, slice

The following object is masked from 'package:purrr':

    reduce

Loading required package: XVector

Attaching package: 'XVector'

The following object is masked from 'package:purrr':

    compact


Attaching package: 'Biostrings'

The following object is masked from 'package:base':

    strsplit

Loading required package: Rsamtools
Loading required package: GenomeInfoDb
Loading required package: GenomicRanges
Loading required package: GenomicAlignments
Loading required package: SummarizedExperiment
Loading required package: Biobase
Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.

Loading required package: DelayedArray
Loading required package: matrixStats

Attaching package: 'matrixStats'

The following objects are masked from 'package:Biobase':

    anyMissing, rowMedians

The following object is masked from 'package:dplyr':

    count


Attaching package: 'DelayedArray'

The following objects are masked from 'package:matrixStats':

    colMaxs, colMins, colRanges, rowMaxs, rowMins, rowRanges

The following object is masked from 'package:purrr':

    simplify

The following objects are masked from 'package:base':

    aperm, apply, rowsum


Attaching package: 'GenomicAlignments'

The following object is masked from 'package:dplyr':

    last


Attaching package: 'ShortRead'

The following object is masked from 'package:dplyr':

    id

The following object is masked from 'package:purrr':

    compose

The following object is masked from 'package:tibble':

    view

> library(Biostrings)
> library(openxlsx)
> #setDadaOpt(BAND_SIZE=32) # for ITS
> path <- getwd()  
> list.files(path)
 [1] "DADA2_AS"                                                 
 [2] "DADA2_SS"                                                 
 [3] "ES1A_S2_L001_R1_001.fastq"                                
 [4] "ES1A_S2_L001_R2_001.fastq"                                
 [5] "ES1A_map_FWD.fasta"                                       
 [6] "ES1A_map_REV.fasta"                                       
 [7] "ES1B_S1_L001_R1_001.fastq"                                
 [8] "ES1B_S1_L001_R2_001.fastq"                                
 [9] "ES1B_map_FWD.fasta"                                       
[10] "ES1B_map_REV.fasta"                                       
[11] "batch_file.txt"                                           
[12] "script_1_demultiplex_cutadapt_2_7.sh"                     
[13] "script_2_remove_demult_files_with_very_low_read_number.sh"
[14] "script_3_dependency_R_code.R"                             
[15] "script_3_run_DADA2_v1.12.sh"                              
[16] "script_4_parallel_itsx_fungi.sh"                          
[17] "script_5_VSEARCH_cluster.sh"                              
[18] "script_6_LULU.sh"                                         
[19] "script_6_R_code_LULU.R"                                   
[20] "script_7.2_Blast_and_match_tax_ready_for_F_guild.sh"      
[21] "script_7.2_matching_blast_with_OTU_table_for_funguild.R"  
[22] "script_7_Blast_and_match.sh"                              
[23] "script_7_matching_blast_with_OTU_table.R"                 
[24] "script_8_FunGUILD.sh"                                     
[25] "script_9_FUNGUILD_to_FINAL_OTU_table.R"                   
[26] "script_9_match_OTU_table_with_tax_guild.sh"               
[27] "slurm-1536643.out"                                        
[28] "slurm-1544028.out"                                        
[29] "slurm-1553036.out"                                        
[30] "slurm_script_1_dem.sh"                                    
[31] "slurm_script_2_runDADA2_v1.12.sh"                         
[32] "uncut_fasta"                                              
> ########################## SENSUS - SS analyses #########
> ##########################
> path.SS <- file.path(path, "DADA2_SS")
> list.files(path.SS)
 [1] "S001_SS_R1.fastq.gz"       "S001_SS_R2.fastq.gz"      
 [3] "S002_SS_R1.fastq.gz"       "S002_SS_R2.fastq.gz"      
 [5] "S003_SS_R1.fastq.gz"       "S003_SS_R2.fastq.gz"      
 [7] "S004_SS_R1.fastq.gz"       "S004_SS_R2.fastq.gz"      
 [9] "S049_SS_R1.fastq.gz"       "S049_SS_R2.fastq.gz"      
[11] "S050_SS_R1.fastq.gz"       "S050_SS_R2.fastq.gz"      
[13] "S051_SS_R1.fastq.gz"       "S051_SS_R2.fastq.gz"      
[15] "S052_SS_R1.fastq.gz"       "S052_SS_R2.fastq.gz"      
[17] "SS_log.txt"                "samples_with_few_reads_SS"
> fnFs <- sort(list.files(path.SS, pattern = "_R1.fastq", full.names = TRUE))    
> fnRs <- sort(list.files(path.SS, pattern = "_R2.fastq", full.names = TRUE))	
> #### set the samples names #######
> sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
> sample.names
[1] "S001" "S002" "S003" "S004" "S049" "S050" "S051" "S052"
> 
> ##### look at the primers in the reads ###########
> 
> FWD <-"AAAATTTGGGCCC"    #INSERT_FWD_PRIMER
> REV <- "AAATTTGGGCCC"    #INSERT_REV_PRIMER
> 
> allOrients <- function(primer) { # Create all orientations of the input sequence
+   require(Biostrings)
+   dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
+   orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
+                RevComp = reverseComplement(dna))
+   return(sapply(orients, toString))  # Convert back to character vector
+ }
> FWD.orients <- allOrients(FWD)
> REV.orients <- allOrients(REV)
> FWD.orients
        Forward      Complement         Reverse         RevComp 
"AAAATTTGGGCCC" "TTTTAAACCCGGG" "CCCGGGTTTAAAA" "GGGCCCAAATTTT" 
> REV.orients
       Forward     Complement        Reverse        RevComp 
"AAATTTGGGCCC" "TTTAAACCCGGG" "CCCGGGTTTAAA" "GGGCCCAAATTT" 
> 
> ### check for primers in sequences ####
> primerHits <- function(primer, fn) {
+   # Counts number of reads in which the primer is found
+   nhits <- vcountPattern(primer, sread(readFastq(fn)))
+   return(sum(nhits > 0))
+ }
> 
> ### in the demultiplexed files SS ####
> Primers.SS<-rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]), 
+                   FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]), 
+                   REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]), 
+                   REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))
> Primers.SS
                 Forward Complement Reverse RevComp
FWD.ForwardReads       0          0       0       0
FWD.ReverseReads       0          0       0       0
REV.ForwardReads       0          0       0       0
REV.ReverseReads       0          0       0       0
> tbname_Primers.SS <- file.path(path,"Primers.in.SS.table")
> write.table(Primers.SS,tbname_Primers.SS, sep = "\t", col.names = NA,row.names = T, quote = F)
> 
> plotQualityProfile(fnFs[1:4])
Scale for 'y' is already present. Adding another scale for 'y', which will
replace the existing scale.
> ggsave("FWD.raw_qualityProfile.SS.pdf", width = 18, height = 14, units = c("in"))
> plotQualityProfile(fnRs[1:4])
Scale for 'y' is already present. Adding another scale for 'y', which will
replace the existing scale.
> ggsave("REV.raw_qualityProfile.SS.pdf", width = 18, height = 14, units = c("in"))
> 
> ### filter the SS reads
> 
> filtFs <- file.path(path.SS, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
> filtRs <- file.path(path.SS, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
> 
> out.SS <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(200, 200),
+                         maxN=0, maxEE=c(1,1), truncQ=2, trimLeft =c(0,0), 
+                         rm.phix=TRUE, compress=TRUE, matchIDs = T, 
+                         multithread=T)
Creating output directory: /cluster/work/users/anderkkr/33_zazzy/example/DADA2_SS/filtered
> 
> ##### the analyses #######
> errF <- learnErrors(filtFs, verbose=TRUE, multithread=T)
28055600 total bases in 140278 reads from 8 samples will be used for learning the error rates.
Initializing error rates to maximum possible estimate.
selfConsist step 1 ........
   selfConsist step 2
   selfConsist step 3
   selfConsist step 4
   selfConsist step 5
Convergence after  5  rounds.
> errR <- learnErrors(filtRs, verbose=TRUE, multithread=T)
28055600 total bases in 140278 reads from 8 samples will be used for learning the error rates.
Initializing error rates to maximum possible estimate.
selfConsist step 1 ........
   selfConsist step 2
   selfConsist step 3
   selfConsist step 4
   selfConsist step 5
Convergence after  5  rounds.
> 
> plotErrors(errF, nominalQ=TRUE)
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> ggsave("error_model_FWD.SS.pdf")
Saving 7 x 7 in image
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> plotErrors(errR, nominalQ=TRUE)
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> ggsave("error_model_REV.SS.pdf")
Saving 7 x 7 in image
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> 
> dadaFs <- dada(filtFs, err=errF, pool = "pseudo", multithread=T)
Sample 1 - 24287 reads in 5121 unique sequences.
Sample 2 - 19558 reads in 3153 unique sequences.
Sample 3 - 10000 reads in 2004 unique sequences.
Sample 4 - 18621 reads in 2877 unique sequences.
Sample 5 - 16863 reads in 2716 unique sequences.
Sample 6 - 16104 reads in 3079 unique sequences.
Sample 7 - 17158 reads in 3189 unique sequences.
Sample 8 - 17687 reads in 2753 unique sequences.

   selfConsist step 2> dadaRs <- dada(filtRs, err=errR, pool = "pseudo", multithread=T)
Sample 1 - 24287 reads in 7889 unique sequences.
Sample 2 - 19558 reads in 5116 unique sequences.
Sample 3 - 10000 reads in 3005 unique sequences.
Sample 4 - 18621 reads in 4945 unique sequences.
Sample 5 - 16863 reads in 4077 unique sequences.
Sample 6 - 16104 reads in 4651 unique sequences.
Sample 7 - 17158 reads in 4844 unique sequences.
Sample 8 - 17687 reads in 4473 unique sequences.

   selfConsist step 2> names(dadaFs) <- sample.names
> names(dadaRs) <- sample.names
> mergers.SS <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, trimOverhang=T, verbose=TRUE)
18440 paired-reads (in 291 unique pairings) successfully merged out of 22873 (in 902 pairings) input.
16246 paired-reads (in 243 unique pairings) successfully merged out of 18574 (in 757 pairings) input.
8357 paired-reads (in 184 unique pairings) successfully merged out of 9348 (in 312 pairings) input.
Duplicate sequences in merged output.
16008 paired-reads (in 319 unique pairings) successfully merged out of 17785 (in 568 pairings) input.
14004 paired-reads (in 210 unique pairings) successfully merged out of 15995 (in 451 pairings) input.
13412 paired-reads (in 242 unique pairings) successfully merged out of 15273 (in 592 pairings) input.
14298 paired-reads (in 271 unique pairings) successfully merged out of 16172 (in 738 pairings) input.
14718 paired-reads (in 218 unique pairings) successfully merged out of 16766 (in 558 pairings) input.
> seqtab.SS <- makeSequenceTable(mergers.SS)
Duplicate sequences detected and merged.
> seqtab.nochim.SS <- removeBimeraDenovo(seqtab.SS, method="consensus",multithread=TRUE, verbose=TRUE)
Identified 376 bimeras out of 996 input sequences.
> #####
> 1-sum(seqtab.nochim.SS)/sum(seqtab.SS)
[1] 0.05987028
> stSS <- file.path(path,"seqtab_SS")
> stnsSS <- file.path(path,"seqtab.nochim_SS")
> saveRDS(seqtab.SS,stSS)
> saveRDS(seqtab.nochim.SS,stnsSS)
> 
> getN <- function(x) sum(getUniques(x))
> track.SS <- cbind(out.SS, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers.SS, getN), rowSums(seqtab.nochim.SS))
Duplicate sequences detected and merged.
> colnames(track.SS) <- c("input.SS", "filtered.SS", "denoisedF.SS", "denoisedR.SS", "merged.SS", "nonchim.SS")
> rownames(track.SS) <- sample.names
> track.SS
     input.SS filtered.SS denoisedF.SS denoisedR.SS merged.SS nonchim.SS
S001    27202       24287        23587        23261     18440      17682
S002    21797       19558        19144        18802     16246      15205
S003    11080       10000         9652         9565      8357       8157
S004    20632       18621        18289        17975     16008      13176
S049    19015       16863        16442        16237     14004      13862
S050    18060       16104        15651        15519     13412      12827
S051    19479       17158        16696        16431     14298      13416
S052    19810       17687        17214        17060     14718      14244
> 
> ########################## Anti-SENSUS - AS analyses #########
> ##########################
> path.AS <- file.path(path, "DADA2_AS")
> list.files(path.AS)
 [1] "AS_log.txt"                "S001_AS_R1.fastq.gz"      
 [3] "S001_AS_R2.fastq.gz"       "S002_AS_R1.fastq.gz"      
 [5] "S002_AS_R2.fastq.gz"       "S003_AS_R1.fastq.gz"      
 [7] "S003_AS_R2.fastq.gz"       "S004_AS_R1.fastq.gz"      
 [9] "S004_AS_R2.fastq.gz"       "S049_AS_R1.fastq.gz"      
[11] "S049_AS_R2.fastq.gz"       "S050_AS_R1.fastq.gz"      
[13] "S050_AS_R2.fastq.gz"       "S051_AS_R1.fastq.gz"      
[15] "S051_AS_R2.fastq.gz"       "S052_AS_R1.fastq.gz"      
[17] "S052_AS_R2.fastq.gz"       "samples_with_few_reads_AS"
> fnFs <- sort(list.files(path.AS, pattern = "_R2.fastq", full.names = TRUE)) ### R1 - should be the FORWARD reads - this pattern might change depending on the files outputted from the demultiplexing step
> fnRs <- sort(list.files(path.AS, pattern = "_R1.fastq", full.names = TRUE))	### R2 - should be the REVERSE reads - this pattern might change depending on the files outputted from the demultiplexing step
> #### set the samples names #######
> sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
> sample.names
[1] "S001" "S002" "S003" "S004" "S049" "S050" "S051" "S052"
> 
> ##### look at the primers in the reads ###########
> allOrients <- function(primer) { # Create all orientations of the input sequence
+   require(Biostrings)
+   dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
+   orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
+                RevComp = reverseComplement(dna))
+   return(sapply(orients, toString))  # Convert back to character vector
+ }
> FWD.orients <- allOrients(FWD)
> REV.orients <- allOrients(REV)
> FWD.orients
        Forward      Complement         Reverse         RevComp 
"AAAATTTGGGCCC" "TTTTAAACCCGGG" "CCCGGGTTTAAAA" "GGGCCCAAATTTT" 
> REV.orients
       Forward     Complement        Reverse        RevComp 
"AAATTTGGGCCC" "TTTAAACCCGGG" "CCCGGGTTTAAA" "GGGCCCAAATTT" 
> 
> ### check for primers in sequences ####
> primerHits <- function(primer, fn) {
+   # Counts number of reads in which the primer is found
+   nhits <- vcountPattern(primer, sread(readFastq(fn)))
+   return(sum(nhits > 0))
+ }
> 
> ### in the demultiplexed files SS ####
> Primers.AS<-rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]), 
+                   FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]), 
+                   REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]), 
+                   REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))
> Primers.AS
                 Forward Complement Reverse RevComp
FWD.ForwardReads       0          0       0       0
FWD.ReverseReads       0          0       0       0
REV.ForwardReads       0          0       0       0
REV.ReverseReads       0          0       0       0
> tbname_Primers.AS <- file.path(path,"Primers.in.AS.table")
> write.table(Primers.AS,tbname_Primers.AS, sep = "\t", col.names = NA,row.names = T, quote = F)
> 
> filtFs <- file.path(path.AS, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
> filtRs <- file.path(path.AS, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
> 
> out.AS <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(200, 200),
+                         maxN=0, maxEE=c(1,1), truncQ=2, trimLeft =c(0,0), 
+                         rm.phix=TRUE, compress=TRUE, matchIDs = T, 
+                         multithread=T)
Creating output directory: /cluster/work/users/anderkkr/33_zazzy/example/DADA2_AS/filtered
> 
> ##### the analyses #######
> errF <- learnErrors(filtFs, verbose=TRUE, multithread=T)
30402800 total bases in 152014 reads from 8 samples will be used for learning the error rates.
Initializing error rates to maximum possible estimate.
selfConsist step 1 ........
   selfConsist step 2
   selfConsist step 3
   selfConsist step 4
   selfConsist step 5
Convergence after  5  rounds.
> errR <- learnErrors(filtRs, verbose=TRUE, multithread=T)
30402800 total bases in 152014 reads from 8 samples will be used for learning the error rates.
Initializing error rates to maximum possible estimate.
selfConsist step 1 ........
   selfConsist step 2
   selfConsist step 3
   selfConsist step 4
   selfConsist step 5
Convergence after  5  rounds.
> plotErrors(errF, nominalQ=TRUE)
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> ggsave("error_model_FWD.AS.pdf")
Saving 7 x 7 in image
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> plotErrors(errR, nominalQ=TRUE)
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> ggsave("error_model_REV.AS.pdf")
Saving 7 x 7 in image
Warning messages:
1: Transformation introduced infinite values in continuous y-axis 
2: Transformation introduced infinite values in continuous y-axis 
> dadaFs <- dada(filtFs, err=errF, pool = "pseudo", multithread=T)
Sample 1 - 29361 reads in 11491 unique sequences.
Sample 2 - 22104 reads in 6506 unique sequences.
Sample 3 - 11168 reads in 3583 unique sequences.
Sample 4 - 20109 reads in 5557 unique sequences.
Sample 5 - 17463 reads in 4670 unique sequences.
Sample 6 - 16317 reads in 5002 unique sequences.
Sample 7 - 17676 reads in 5458 unique sequences.
Sample 8 - 17816 reads in 4883 unique sequences.

   selfConsist step 2> dadaRs <- dada(filtRs, err=errR, pool = "pseudo", multithread=T)
Sample 1 - 29361 reads in 5495 unique sequences.
Sample 2 - 22104 reads in 3628 unique sequences.
Sample 3 - 11168 reads in 2094 unique sequences.
Sample 4 - 20109 reads in 3194 unique sequences.
Sample 5 - 17463 reads in 2424 unique sequences.
Sample 6 - 16317 reads in 2829 unique sequences.
Sample 7 - 17676 reads in 3183 unique sequences.
Sample 8 - 17816 reads in 2538 unique sequences.

   selfConsist step 2> names(dadaFs) <- sample.names
> names(dadaRs) <- sample.names
> mergers.AS <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, trimOverhang=T, verbose=TRUE)
20544 paired-reads (in 358 unique pairings) successfully merged out of 26878 (in 1107 pairings) input.
17902 paired-reads (in 329 unique pairings) successfully merged out of 20872 (in 1205 pairings) input.
9306 paired-reads (in 209 unique pairings) successfully merged out of 10277 (in 369 pairings) input.
Duplicate sequences in merged output.
16879 paired-reads (in 426 unique pairings) successfully merged out of 19042 (in 912 pairings) input.
14578 paired-reads (in 230 unique pairings) successfully merged out of 16522 (in 520 pairings) input.
13657 paired-reads (in 284 unique pairings) successfully merged out of 15281 (in 738 pairings) input.
14194 paired-reads (in 287 unique pairings) successfully merged out of 16500 (in 1020 pairings) input.
14834 paired-reads (in 265 unique pairings) successfully merged out of 16888 (in 787 pairings) input.
> seqtab.AS <- makeSequenceTable(mergers.AS)
Duplicate sequences detected and merged.
> seqtab.nochim.AS <- removeBimeraDenovo(seqtab.AS, method="consensus",multithread=TRUE, verbose=TRUE)
Identified 674 bimeras out of 1250 input sequences.
> 
> stAS <- file.path(path,"seqtab_AS")
> stnsAS <- file.path(path,"seqtab.nochim_AS")
> saveRDS(seqtab.AS,stAS)
> saveRDS(seqtab.nochim.AS,stnsAS)
> 
> getN <- function(x) sum(getUniques(x))
> track.AS <- cbind(out.AS, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers.AS, getN), rowSums(seqtab.nochim.AS))
Duplicate sequences detected and merged.
> colnames(track.AS) <- c("input.AS", "filtered.AS", "denoisedF.AS", "denoisedR.AS", "merged.AS", "nonchim.AS")
> rownames(track.AS) <- sample.names
> 
> track.AS
     input.AS filtered.AS denoisedF.AS denoisedR.AS merged.AS nonchim.AS
S001    33851       29361        27029        28920     20544      19561
S002    25545       22104        21049        21736     17902      16456
S003    12838       11168        10358        10974      9306       8953
S004    23208       20109        19133        19901     16879      13311
S049    20285       17463        16595        17242     14578      14260
S050    19143       16317        15380        16076     13657      12945
S051    20929       17676        16645        17354     14194      13074
S052    21006       17816        17003        17552     14834      14042
> track.SS
     input.SS filtered.SS denoisedF.SS denoisedR.SS merged.SS nonchim.SS
S001    27202       24287        23587        23261     18440      17682
S002    21797       19558        19144        18802     16246      15205
S003    11080       10000         9652         9565      8357       8157
S004    20632       18621        18289        17975     16008      13176
S049    19015       16863        16442        16237     14004      13862
S050    18060       16104        15651        15519     13412      12827
S051    19479       17158        16696        16431     14298      13416
S052    19810       17687        17214        17060     14718      14244
> write.xlsx(track.AS, "track_reads_AS.xlsx")
> write.xlsx(track.SS, "track_reads_SS.xlsx")
> 
> 
> #Define a function for combining two or more tables, collapsing samples with similar names:  
> sumSequenceTables <- function(table1, table2, ..., orderBy = "abundance") {
+   # Combine passed tables into a list
+   tables <- list(table1, table2)
+   tables <- c(tables, list(...))
+   # Validate tables
+   if(!(all(sapply(tables, dada2:::is.sequence.table)))) {
+     stop("At least two valid sequence tables, and no invalid objects, are expected.")
+   }
+   sample.names <- rownames(tables[[1]])
+   for(i in seq(2, length(tables))) {
+     sample.names <- c(sample.names, rownames(tables[[i]]))
+   }
+   seqs <- unique(c(sapply(tables, colnames), recursive=TRUE))
+   sams <- unique(sample.names)
+   # Make merged table
+   rval <- matrix(0L, nrow=length(sams), ncol=length(seqs))
+   rownames(rval) <- sams
+   colnames(rval) <- seqs
+   for(tab in tables) {
+     rval[rownames(tab), colnames(tab)] <- rval[rownames(tab), colnames(tab)] + tab
+   }
+   # Order columns
+   if(!is.null(orderBy)) {
+     if(orderBy == "abundance") {
+       rval <- rval[,order(colSums(rval), decreasing=TRUE),drop=FALSE]
+     } else if(orderBy == "nsamples") {
+       rval <- rval[,order(colSums(rval>0), decreasing=TRUE),drop=FALSE]
+     }
+   }
+   rval
+ }
> 
> stAS <- file.path(path,"seqtab_AS")
> stnsAS <- file.path(path,"seqtab.nochim_AS")
> stSS <- file.path(path,"seqtab_SS")
> stnsSS <- file.path(path,"seqtab.nochim_SS")
> seqtab.nochim.AS <- readRDS(stnsAS)
> seqtab.nochim.SS <- readRDS(stnsSS)
> seqtab_AS <- readRDS(stAS)
> seqtab_SS <- readRDS(stSS)
> Both_sumtable_with_chim <- sumSequenceTables(seqtab_SS,seqtab_AS)
> Both_nochim_sumtable <- sumSequenceTables(seqtab.nochim.SS,seqtab.nochim.AS)
> stBoth <- file.path(path,"seqtab.with_Chim_Both")
> stnsBoth <- file.path(path,"seqtab.nochim_Both")
> saveRDS(Both_sumtable_with_chim,stBoth)
> saveRDS(Both_nochim_sumtable,stnsBoth)
> 
> #### get and save read length
> reads.per.seqlen.no.chim <- tapply(colSums(Both_nochim_sumtable), factor(nchar(getSequences(Both_nochim_sumtable))), sum)
> df.without.chim <- data.frame(length=as.numeric(names(reads.per.seqlen.no.chim)), count=reads.per.seqlen.no.chim)
> write.xlsx(df.without.chim, "No_chimeras_seq_length_table.xlsx")
> df.without.chim$count<-as.numeric(df.without.chim$count)
> ggplot(data=df.without.chim, aes(x=length, y=count)) + geom_col()
> ggsave("ASV_no_chime_length.pdf", width = 18, height = 14, units = c("in"))
> 
> #### get all reads counted
> track.SS<-as.data.frame(track.SS) %>% 
+   rownames_to_column("sample.names")
> track.SS[is.na(track.SS)]<-0
> track.AS<-as.data.frame(track.AS) %>% 
+   rownames_to_column("sample.names")
> track.AS[is.na(track.AS)]<-0
> 
> track_merged_data<-cbind(rowSums(Both_sumtable_with_chim), rowSums(sign(Both_sumtable_with_chim)), rowSums(Both_nochim_sumtable), rowSums(sign(Both_nochim_sumtable)))
> rownames(track_merged_data) <- sample.names
> colnames(track_merged_data) <- c("Reads_w_chim","ASVs_w_chim" ,"Reads_nonchim", "ASVs_nonchim")
> track_merged_data<-as.data.frame(track_merged_data) %>% 
+   rownames_to_column("sample.names")
> track_merged_data[is.na(track_merged_data)]<-0
> 
> track_both<-track.SS %>% 
+   full_join(track.AS) %>%
+   full_join(track_merged_data) %>% 
+   replace(., is.na(.), 0) %>% 
+   mutate(input=(input.SS+input.AS), denoisedF=(denoisedF.SS+denoisedF.AS), 
+          denoisedR=(denoisedR.SS+denoisedR.AS)) %>% 
+   select(sample.names,input, denoisedF, denoisedR, Reads_w_chim, Reads_nonchim, ASVs_w_chim, ASVs_nonchim)
Joining, by = "sample.names"
Joining, by = "sample.names"
> 
> write.xlsx(track_both,"track_reads_both.xlsx")
> 
> #Transpose table, assign names, extract sequences and saving table, for further processing:
> trasBoth_nochim_sumtable <- as.data.frame(t(Both_nochim_sumtable))
> #Get DNA sequences
> sequences <- row.names(trasBoth_nochim_sumtable)
> #Assign new rownames
> row.names(trasBoth_nochim_sumtable) <- paste0("seq",seq.int(nrow(trasBoth_nochim_sumtable)))
> tbname <- file.path(path,"DADA2_noChime.table")
> {write.table(trasBoth_nochim_sumtable,tbname,sep="\t",col.names = NA, quote=FALSE)}
> #Extract OTUs (sequences)
> sinkname <- file.path(path,"DADA2_noChime_raw.otus")
> {
+   sink(sinkname)
+   for (seqX in seq.int(nrow(trasBoth_nochim_sumtable))) {
+     header <- paste0(">","seq",seqX,"\n")
+     cat(header)
+     seqq <- paste0(sequences[seqX],"\n")
+     cat(seqq)
+   }
+   sink()
+ }
> 
> #Define function to extract sequences sample-wise
> extrSamDADA2 <- function(my_table) {
+   out_path <- file.path(path, "DADA2_extracted_samples_no_chim")
+   if(!file_test("-d", out_path)) dir.create(out_path)
+   for (sampleX in seq(1:dim(my_table)[1])){
+     sinkname <- file.path(out_path, paste0(rownames(my_table)[sampleX],".fas"))
+     {
+       sink(sinkname)
+       for (seqX in seq(1:dim(my_table)[2])) {
+         if (my_table[sampleX,seqX] > 0) {
+           header <- paste0(">",rownames(my_table)[sampleX],";size=",my_table[sampleX,seqX],";","\n")
+           cat(header)
+           seqq <- paste0(colnames(my_table)[seqX],"\n")
+           cat(seqq)
+         }
+       }
+       sink()
+     }
+   }
+ }
> 
> #Extract samplewise sequences from the non-chimera table using the above function:
> extrSamDADA2(Both_nochim_sumtable)
> 
> sink("sessionInfo.txt")
> sessionInfo()
> sink()
> 

Task and CPU usage stats:
       JobID    JobName  AllocCPUS   NTasks     MinCPU MinCPUTask     AveCPU    Elapsed ExitCode 
------------ ---------- ---------- -------- ---------- ---------- ---------- ---------- -------- 
1553036          demult          4                                             00:05:22      0:0 
1553036.bat+      batch          4        1   00:13:12          0   00:13:12   00:05:22      0:0 
1553036.ext+     extern          4        1   00:00:00          0   00:00:00   00:05:22      0:0 

Memory usage stats:
       JobID     MaxRSS MaxRSSTask     AveRSS MaxPages   MaxPagesTask   AvePages 
------------ ---------- ---------- ---------- -------- -------------- ---------- 
1553036                                                                          
1553036.bat+   1302428K          0   1302428K      344              0        344 
1553036.ext+          0          0          0        0              0          0 

Disk usage stats:
       JobID  MaxDiskRead MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteTask   AveDiskWrite 
------------ ------------ --------------- -------------- ------------ ---------------- -------------- 
1553036                                                                                               
1553036.bat+        5.93M               0          5.93M        0.09M                0          0.09M 
1553036.ext+        0.00M               0          0.00M            0                0              0 

Job 1553036 completed at Thu Nov 19 15:36:53 CET 2020
